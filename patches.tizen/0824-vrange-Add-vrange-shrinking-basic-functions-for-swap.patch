From 660c620704b390e9f2433805c63debf336e8f005 Mon Sep 17 00:00:00 2001
From: Minchan Kim <minchan@kernel.org>
Date: Thu, 25 Jul 2013 13:44:42 +0900
Subject: [PATCH 0824/1302] vrange: Add vrange shrinking basic functions for
 swapless system

This patch makes vrange works on swapless system by using slab
shrinker.

The reason I use shrinker is that Dave and Glauber are trying to
make slab shrinker being aware of node/memcg so if the patchset
reach on mainline, we also can support node/memcg in vrange, easily.

Another reason I selected slab shrinker is that normally slab shrinker
is called after normal reclaim of file-backed page(ex, page cache)
so reclaiming preference would be this, I expect.(TODO: invstigate
and might need more tunes in reclaim path)

        page cache -> vrange by slab shrinking -> anon page

It does make sense because page cache can have stream data so there is
no point to shrink vrange pages if there are lots of streaming pages
in page cache.

In this version, I didn't check it works well but it's design concept
so we can make it work via modify page reclaim path.
I will have more experiment.

One of disadvantage with using slab shrink is that slab shrinker isn't
called in using memcg so memcg-noswap system cannot take advantage of it.
Hmm, Maybe I will jump into relcaim code to hook some point to control
vrange page shrinking more freely.

XXX: isolate_vrange maybe should be vrange_isolate?
XXX: Sanity check function names
XXX: _vrange_get only has one user?

Signed-off-by: Minchan Kim <minchan@kernel.org>
Signed-off-by: John Stultz <john.stultz@linaro.org>
Signed-off-by: MyungJoo Ham <myungjoo.ham@samsung.com>
---
 mm/vrange.c | 94 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++--
 1 file changed, 91 insertions(+), 3 deletions(-)

diff --git a/mm/vrange.c b/mm/vrange.c
index 7af4584..b93d469 100644
--- a/mm/vrange.c
+++ b/mm/vrange.c
@@ -26,10 +26,19 @@ static inline unsigned int vrange_size(struct vrange *range)
 	return range->node.last + 1 - range->node.start;
 }
 
+static int shrink_vrange(struct shrinker *s, struct shrink_control *sc);
+
+static struct shrinker vrange_shrinker = {
+	.shrink = shrink_vrange,
+	.seeks = DEFAULT_SEEKS
+};
+
 static int __init vrange_init(void)
 {
 	INIT_LIST_HEAD(&vrange_list.list);
 	mutex_init(&vrange_list.lock);
+
+	register_shrinker(&vrange_shrinker);
 	vroot_cachep = kmem_cache_create("vrange_root",
 				sizeof(struct vrange_root), 0,
 				SLAB_DESTROY_BY_RCU|SLAB_PANIC, NULL);
@@ -213,9 +222,14 @@ static void __vrange_free(struct vrange *range)
 static inline void __vrange_lru_add(struct vrange *range)
 {
 	mutex_lock(&vrange_list.lock);
-	WARN_ON(!list_empty(&range->lru));
-	list_add(&range->lru, &vrange_list.list);
-	vrange_list.size += vrange_size(range);
+	/*
+	 * We need this check because it could be raced with
+	 * shrink_vrange and vrange_resize
+	 */
+	if (list_empty(&range->lru)) {
+		list_add(&range->lru, &vrange_list.list);
+		vrange_list.size += vrange_size(range);
+	}
 	mutex_unlock(&vrange_list.lock);
 }
 
@@ -239,6 +253,14 @@ static void __vrange_add(struct vrange *range, struct vrange_root *vroot)
 	__vrange_lru_add(range);
 }
 
+static inline int __vrange_get(struct vrange *vrange)
+{
+	if (!atomic_inc_not_zero(&vrange->refcount))
+		return 0;
+
+	return 1;
+}
+
 static inline void __vrange_put(struct vrange *range)
 {
 	if (atomic_dec_and_test(&range->refcount)) {
@@ -874,3 +896,69 @@ int discard_vpage(struct page *page)
 
 	return 1;
 }
+
+static struct vrange *isolate_vrange(void)
+{
+	struct vrange *vrange = NULL;
+	mutex_lock(&vrange_list.lock);
+	while (!list_empty(&vrange_list.list)) {
+		vrange = list_entry(vrange_list.list.prev,
+				struct vrange, lru);
+		list_del_init(&vrange->lru);
+		vrange_list.size -= vrange_size(vrange);
+
+		/* vrange is going to destroy */
+		if (__vrange_get(vrange))
+			break;
+
+		vrange = NULL;
+	}
+
+	mutex_unlock(&vrange_list.lock);
+	return vrange;
+}
+
+static unsigned int discard_vrange(struct vrange *vrange)
+{
+	return 0;
+}
+
+static int shrink_vrange(struct shrinker *s, struct shrink_control *sc)
+{
+	struct vrange *range = NULL;
+	long nr_to_scan = sc->nr_to_scan;
+	long size = vrange_list.size;
+
+        /* current is dying so it will release memory soon */
+        if (fatal_signal_pending(current))
+                return -1;
+
+	if (!nr_to_scan)
+		return size;
+
+	if (sc->nr_to_scan && !(sc->gfp_mask & __GFP_IO))
+		return -1;
+
+	while (size > 0 && nr_to_scan > 0) {
+		range = isolate_vrange();
+		if (!range)
+			break;
+
+		/* range is removing so don't bother */
+		if (!range->owner) {
+			__vrange_put(range);
+			size -= vrange_size(range);
+			nr_to_scan -= vrange_size(range);
+			continue;
+		}
+
+		if (discard_vrange(range) < 0)
+			__vrange_lru_add(range);
+		__vrange_put(range);
+
+		size -= vrange_size(range);
+		nr_to_scan -= vrange_size(range);
+	}
+
+	return size;
+}
-- 
1.8.3.2

